{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from socialreaper import Twitter\n",
        "from socialreaper.tools import to_csv\n",
        "\n",
        "twt = Twitter(app_key=\"xxx\", app_secret=\"xxx\", oauth_token=\"xxx\", \n",
        "    oauth_token_secret=\"xxx\")\n",
        "    \n",
        "tweets = twt.user(\"realDonaldTrump\", count=500, exclude_replies=True, \n",
        "    include_retweets=False)\n",
        "    \n",
        "to_csv(list(tweets), filename='trump.csv')\n",
        "\n",
        "# Define the social media platform and the search term\n",
        "platform = \"twitter\"\n",
        "term = \"hp printer\"\n",
        "\n",
        "# Get the search results\n",
        "url = f\"https://api.twitter.com/2/search/tweets?q={term}&count=100\"\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\",\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Parse the response\n",
        "data = json.loads(response.content)\n",
        "\n",
        "# Create a list of posts\n",
        "posts = []\n",
        "for tweet in data[\"data\"]:\n",
        "    posts.append({\n",
        "        \"text\": tweet[\"text\"],\n",
        "        \"date\": tweet[\"created_at\"],\n",
        "        \"author\": tweet[\"author_name\"],\n",
        "        \"location\": tweet[\"location\"],\n",
        "    })\n",
        "\n",
        "# Classify the posts\n",
        "classifier = nlp.load(\"classifier.pkl\")\n",
        "for post in posts:\n",
        "    post[\"brand\"] = classifier.predict(post[\"text\"])[0]\n",
        "    post[\"model\"] = classifier.predict(post[\"text\"])[1]\n",
        "\n",
        "# Detect the features and problems\n",
        "features = [\"ink\", \"paper\", \"wifi\", \"scanner\", \"color\"]\n",
        "problems = [\"jam\", \"low ink\", \"won't connect\", \"pages blank\"]\n",
        "for post in posts:\n",
        "    for feature in features:\n",
        "        if feature in post[\"text\"]:\n",
        "            post[\"features\"].append(feature)\n",
        "    for problem in problems:\n",
        "        if problem in post[\"text\"]:\n",
        "            post[\"problems\"].append(problem)\n",
        "\n",
        "# Identify the sentiments\n",
        "sentiment_analyzer = sentiment.load(\"sentiment.pkl\")\n",
        "for post in posts:\n",
        "    post[\"sentiment\"] = sentiment_analyzer.predict(post[\"text\"])[0]\n",
        "\n",
        "# Create the knowledge graph\n",
        "graph = rdflib.Graph()\n",
        "for post in posts:\n",
        "    graph.add((post[\"brand\"], rdflib.RDF.type, rdflib.RDFS.Class))\n",
        "    graph.add((post[\"model\"], rdflib.RDF.type, rdflib.RDFS.Class))\n",
        "    graph.add((post[\"brand\"], rdflib.RDF.type, rdflib.RDF.Property))\n",
        "    graph.add((post[\"model\"], rdflib.RDF.type, rdflib.RDF.Property))\n",
        "    graph.add((post[\"brand\"], rdflib.RDF.value, post[\"text\"]))\n",
        "    graph.add((post[\"model\"], rdflib.RDF.value, post[\"text\"]))\n",
        "    graph.add((post[\"features\"], rdflib.RDF.type, rdflib.RDFS.Class))\n",
        "    graph.add((post[\"problems\"], rdflib.RDF.type, rdflib.RDFS.Class))\n",
        "    graph.add((post[\"features\"], rdflib.RDF.value, post[\"text\"]))\n",
        "    graph.add((post[\"problems\"], rdflib.RDF.value, post[\"text\"]))\n",
        "    graph.add((post[\"sentiment\"], rdflib.RDF.type, rdflib.RDFS.Class))\n",
        "    graph.add((post[\"sentiment\"], rdflib.RDF.value, post[\"text\"]))\n",
        "\n",
        "# Save the knowledge graph\n",
        "graph.serialize(\"knowledge_graph.rdf\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b14e8d59fd91>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msocialreaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m twt = Twitter(app_key=\"xxx\", app_secret=\"xxx\", oauth_token=\"xxx\", \n\u001b[0m\u001b[1;32m      8\u001b[0m     oauth_token_secret=\"xxx\")\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Twitter.__init__() got an unexpected keyword argument 'app_key'"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LH14BwLCSDx5",
        "outputId": "db8020b0-642b-4c3d-f60a-7512ac23ceec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install socialreaper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYabxpW_ghYZ",
        "outputId": "bb880194-6ee9-45b7-e834-7e36e952a369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting socialreaper\n",
            "  Downloading socialreaper-1.0.3.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from socialreaper) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from socialreaper) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from socialreaper) (3.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.11.1->socialreaper) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.11.1->socialreaper) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.11.1->socialreaper) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.11.1->socialreaper) (3.4)\n",
            "Building wheels for collected packages: socialreaper\n",
            "  Building wheel for socialreaper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for socialreaper: filename=socialreaper-1.0.3-py3-none-any.whl size=203986 sha256=8117b67b6f9cb6362912fa2edd98604f698e53833d0df1c573aa56bb9ff4365c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/bc/80/d41f83956d29b6d0992fc184d7552650be0b234495a95232c0\n",
            "Successfully built socialreaper\n",
            "Installing collected packages: socialreaper\n",
            "Successfully installed socialreaper-1.0.3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}